{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4_Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FranziskaSW/DS-keyword-clusters/blob/master/4_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcdJ4VeJrOez",
        "colab_type": "text"
      },
      "source": [
        "# Analyzing The Network Of Keywords\n",
        "## Motivation\n",
        "Every article in the NYT is tagged with several keywords. We can assume that some of these keywords are \"section-specific\", i.e. they mostly appear in specific sections (for example, tennis players will mostly appear in the Sports section, while dancers would appear in Culture). But some keywords might appear in many different sections, (for example, 'New York' - it's the New York Times after all).\n",
        "\n",
        "Simplyfied our dataset looks like this:\n",
        "\n",
        "| article   | keywords                                                     | section |\n",
        "| ---- | ------------------------------------------------------------ | ------- |\n",
        "| 1    | 'Nuclear Weapons', 'Trump, Donald J', 'Kim Jong-un', 'North Korea', 'United States Defense and Military Forces' | World   |\n",
        "| 2    | 'Cats', 'Wildfires', 'Santa Rosa (California)'               | U.S.    |\n",
        "| 3    | 'Playoff Games', 'Football', 'Super Bowl', 'New England Patriots', 'Jacksonville Jaguars' | Sports  |\n",
        "| ... |                           ...  |                                         ...  | \n",
        "\n",
        "\n",
        "\n",
        "If we would assign each keyword a section of the newspaper it \"belongs\" to (by the dominant section it appears in), we can visualize how the different sections of the newspaper interact, content-wise. We will define the keywords network with the keywords as nodes, and add an edge between two keywords if they appeared together in an article.\n",
        "\n",
        "We will embed this graph in 2D in a way that represents the co-appearance connections between the keywords, visualizing the relations between the different sections of the newspaper. We will see if this network of keywords truthfully represents the sections of the newspaper.\n",
        "\n",
        "From another perspective, since the embedding of the network is based solely on the connections between keywords, keywords that will be close in the embedding, also have a strong connection in the content of the NYT. In this sense, the network could also reveal some unexpected connections. \n",
        "\n",
        "## Building The Network\n",
        "### Getting & Cleaning The Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgBRB0t2D8wF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import requests\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import chain\n",
        "from itertools import combinations\n",
        "import math\n",
        "import os\n",
        "\n",
        "global cwd\n",
        "cwd = os.getcwd()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gY_HPDyZD2Ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(year, month):\n",
        "    \"\"\"\n",
        "    pulls the meta data of the articles that were published during that month and saves it in archive,\n",
        "    uses nytimes search api\n",
        "    :param year: str\n",
        "    :param month: str\n",
        "    \"\"\"\n",
        "    archive_key = 'Jctp3rj1ZdOaLQiMArs79ioGnwvfK1pC'\n",
        "    month_api = year + '/' + month\n",
        "    if len(month) == 1:\n",
        "        month = '0' + month\n",
        "    data_suffix = year + '_' + month\n",
        "    url = 'https://api.nytimes.com/svc/archive/v1/' + month_api + '.json?api-key=' + archive_key\n",
        "\n",
        "    print('-------------- load', url, ' --------------')\n",
        "    html = requests.get(url)  # load page\n",
        "    a = html.text\n",
        "    api_return = json.loads(a)\n",
        "    articles = api_return['response']['docs']\n",
        "    # articles = response['docs']\n",
        "    df = pd.DataFrame(articles)\n",
        "    with open(cwd + \"/data/archive/articles_\" + data_suffix + \".pickle\", \"wb\") as f:\n",
        "        pickle.dump(df, f)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJQLBH6iEUA5",
        "colab_type": "text"
      },
      "source": [
        "### Data Cleaning\n",
        "\n",
        "**Articles**: \n",
        "\n",
        "Before cleaning the data, the API returns 232172 articles for the three years 2016-2018. But not all of them are relevant to us and some of them are duplicates. We will clean this dataset in the following way:\n",
        "\n",
        "- Only keep articles with more than 20 words. \n",
        "\n",
        "- Drop duplicate articles, keep an article with the same headline only if it appears in different sections\n",
        "\n",
        "- Only articles that belong to the type_of_material 'News' and 'Briefing' (shorter Articles, mainly from the Sports section). Those make about 71% of the plain results that are left if we take all of the API results (after dropping the duplicates and >20 words). \n",
        "\n",
        "- Drop articles of sections that are not relevant for our analysis (as explained in next paragraph about section cleaning)\n",
        "\n",
        "After cleaning we are left with 104953 articles for the three years 2016, 2017, 2018.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0aZ7Gs6EE1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def extr_headline_main(field):\n",
        "    \"\"\"\n",
        "    extracts main headline from api entry\n",
        "    :param field: api entry structure\n",
        "    :return: headline\n",
        "    \"\"\"\n",
        "    return field['main']\n",
        "\n",
        "\n",
        "def clean_articles(df, word_count):\n",
        "    \"\"\"\n",
        "    clean the articles, only keep articles with\n",
        "    - more than 20 words\n",
        "    - that are certain 'type_of_material'\n",
        "    - drop duplicate articles, if same headline appears in same section\n",
        "    :param df: DataFrame of articles\n",
        "    :param word_count: minimum amount of words, not included\n",
        "    :return: cleaned DataFrame of articles\n",
        "    \"\"\"\n",
        "    df = df[~(df.word_count.isnull())]\n",
        "    df['word_count'] = df.word_count.apply(lambda x: int(x))\n",
        "    df = df[df.word_count > word_count]\n",
        "    df['headline'] = df.headline.apply(lambda x: extr_headline_main(x))\n",
        "    df = df.drop_duplicates(['headline', 'section_name'])\n",
        "    mask = ['News', 'Brief', 'briefing']\n",
        "    df = df[df.type_of_material.isin(mask)]\n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23zTBjU5Di1w",
        "colab_type": "text"
      },
      "source": [
        "**Section**: \n",
        "\n",
        "The field \"section_name\" has 98 different values for the years 2016-2018 in the raw data. Since we wish to use the sections of the NYT to tag the nodes of the network, this is too much and we need to manually reduce them to fewer sections while staying true to the NY Times website. We also drop sections that are not interesting for our analysis e.g. 'Crosswords & Games' or 'Insider Events'. We end up with the sections: World, Business&Technology, Culture, Sports, U.S. New York, Leisure, Style, Politics, Health&Science.\n",
        "\n",
        "After this routine, there are still 22.4% articles left, that do not have a section_name tag. Therefore we repeated the same routine on the 'news_board' fild, which contains similar information. Now only  1,8% articles are left that can not be tagged to a section, and these get the tag 'Unknown'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMBVH9v5DozD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getSectionDict(name):\n",
        "    \"\"\"\n",
        "    groups section_name into 12 meta-sections\n",
        "    :param name: section_name in from search api\n",
        "    :return: name of meta-section\n",
        "    \"\"\"\n",
        "    world = ['World', 'Africa', 'Americas', 'Asia', 'Asia Pacific', 'Australia', 'Canada', 'Europe', 'Middle East',\n",
        "             'What in the World', 'Opinion | The World', 'Foreign']\n",
        "    if name in world: return 'World'\n",
        "    us = ['U.S.', 'National']\n",
        "    if name in us: return 'U.S.'\n",
        "    politics = ['Elections', 'Politics', 'Tracking Trumps Agenda', 'The Upshot', 'Opinion | Politics', 'Upshot',\n",
        "                'Washington ']\n",
        "    if name in politics: return 'Politics'\n",
        "    ny = ['N.Y. / Region', 'New York Today', 'Metro', 'Metropolitan']\n",
        "    if name in ny: return 'New York'\n",
        "    business_technology = ['Business Day', 'Economy', 'Media', 'Money', 'DealBook', 'Markets', 'Energy', 'IPhone App',\n",
        "                           'Media', 'Technology', 'Personal Tech', 'Entrepreneurship', 'Your Money', 'Business',\n",
        "                           'SundayBusiness']\n",
        "    if name in business_technology: return 'Business & Technology'\n",
        "    sports = ['Skiing', 'Rugby', 'Sailing', 'Cycling', 'Cricket', 'Auto Racing', 'Horse Racing', 'World Cup',\n",
        "              'Olympics', 'Pro Football', 'Pro Basketball', 'Sports', 'Baseball', 'NFL', 'College Football', 'NBA',\n",
        "              'College Basketball', 'Hockey', 'Soccer', 'Golf', 'Tennis']\n",
        "    if name in sports: return 'Sports'\n",
        "    arts = ['Opinion | Culture', 'Arts', 'Art & Design', 'Books', 'Book Review', 'BookReview', 'Best Sellers',\n",
        "            'By the Book', 'Crime', 'Children\\'s Books', 'Book Review Podcast', 'Now read this', 'Dance', 'Movies',\n",
        "            'Music', 'Television', 'Theater', 'Pop Culture', 'Watching', 'Culture', 'Arts&Leisure']\n",
        "    if name in arts: return 'Culture'\n",
        "    style = ['Men\\'s Style', 'Style', 'Styles', 'TStyle', 'Fashion & Style', 'Fashion & Beauty', 'Fashion', 'Weddings',\n",
        "             'Self-Care']\n",
        "    if name in style: return 'Style'\n",
        "    science = ['Energy & Environment', 'Science', 'Climate', 'Opinion | Environment', 'Space & Cosmos', 'Trilobites',\n",
        "               'Sciencetake', 'Out There']\n",
        "    health = ['Mind', 'Health Guide', 'Health', 'Health Policy', 'Live', 'Global Health', 'The New Old Age', 'Science',\n",
        "              'Well', 'Move']\n",
        "    sci_hel = science + health + ['Family', 'Live']\n",
        "    if name in sci_hel: return 'Health & Science'\n",
        "    food = ['Eat', 'Wine, Beer & Cocktails', 'Restaurant Reviews', 'Dining', 'Food']\n",
        "    travel = ['36 Hours', 'Frugal Traveler', '52 Places to go', 'Travel']\n",
        "    magazine = ['Smarter Living', 'Wirecutter', 'Automobiles', 'T Magazine', 'Magazine', 'Design & Interiors',\n",
        "                'Entertainment', 'Video', 'Weekend']\n",
        "    leisure = food + travel + magazine\n",
        "    if name in leisure: return 'Leisure'\n",
        "    opinion = ['Opinion', 'Letters', 'Contributors', 'Editorials', 'Columnists', 'OpEd', 'Sunday Review', 'Games',\n",
        "               'Editorial']\n",
        "    realestate = ['Real Estate', 'RealEstate', 'Commercial Real Estate', 'The High End', 'Commercial', 'Find a Home',\n",
        "                  'Mortgage Calculator', 'Your Real Estate', 'List a Home']\n",
        "    education = ['Education', 'Education Life', 'The Learning Network', 'Lesson Plans', 'Learning']\n",
        "    delete = (['Blogs', 'Insider Events', 'Retirement', 'AmÃ©rica', 'Multimedia/Photos', 'The Daily',\n",
        "               'Briefing', 'Sunday Review', 'Crosswords & Games', 'Times Insider', 'Corrections', 'NYTNow',\n",
        "               'Corrections', 'Podcasts', 'Insider', 'Obits', 'Summary']\n",
        "              + opinion + education + realestate)\n",
        "    if name in delete: return '*DELETE*'\n",
        "    else: return '*UNKNOWN*'\n",
        "\n",
        "    \n",
        "def clean_sections(df):\n",
        "    \"\"\"\n",
        "    uses getSectionDict to rename sections to their meta-section\n",
        "    :param df: DataFrame of articles\n",
        "    :return: DataFrame of articles, section renamed\n",
        "    \"\"\"\n",
        "    df['section'] = df.section_name.apply(lambda x: getSectionDict(x))\n",
        "    without_section = df[df.section == '*UNKNOWN*']  # the articles that haven't had a section_name,\n",
        "                                                     # many of them have news_desk entry\n",
        "    sections_from_newsdesk = without_section.news_desk.apply(lambda x: getSectionDict(x))\n",
        "    idx = sections_from_newsdesk.index.get_values()\n",
        "    df.loc[idx, 'section'] = sections_from_newsdesk\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PBsAxYEEdZz",
        "colab_type": "text"
      },
      "source": [
        "### Create Keyword Table\n",
        "In order to create a graph of keywords we first need to gather some information about them. We are mainly interested in which section the keyword belongs to and we want to translate every keyword into a keyword id, so that instead of saving the whole string, we would only save a number. \n",
        "The process of creating the table of keywords was more complicated then we first thought it would be, because the DataFrames for the years got very large and the process needed many lookups.\n",
        "\n",
        "For runtime and memory reasons, we processed the articles year-wise and then combined the yearly keyword tables again to find out the final and correct numbers. This is especially the case when we wanted to assign the keywords to sections. For that task, we counted how many times the keyword appeared in each section, and when we actually had the data of the whole timeframe in memory, we checked if one section stands out enough to assign the keyword to it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEvCGPkLFCbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def extr_keywords_step1(field):\n",
        "    \"\"\"\n",
        "    brings entry as it comes from api in more handy format\n",
        "    :param field: 'keywords' entry of api\n",
        "    :return: tupel (name, value)\n",
        "    \"\"\"\n",
        "    keyword = field\n",
        "    keyword_tup = (keyword['name'], keyword['value'])\n",
        "    return keyword_tup\n",
        "\n",
        "\n",
        "def create_keyword_table_partial(df):\n",
        "    \"\"\"\n",
        "    uses article DataFrame to create table of keywords. How often keyword appeared in which section\n",
        "    :param df: articles DataFrame\n",
        "    :return: DataFrame of keywords (keyword, section, counts)\n",
        "    \"\"\"\n",
        "    dfs = df[['_id', 'section', 'pub_date', 'headline', 'keywords']]\n",
        "    # expand columns from keyword_dict\n",
        "    d1 = dfs.keywords.apply(pd.Series).merge(dfs, left_index=True, right_index=True).drop([\"keywords\"], axis=1)\n",
        "    # columns are additional rows\n",
        "    d2 = d1.melt(id_vars=['_id', 'section', 'pub_date', 'headline'], value_name=\"keyword\").drop(\"variable\", axis=1)\n",
        "\n",
        "    mask = d2.keyword.isna()\n",
        "    d3 = d2[~mask]\n",
        "\n",
        "    d3 = d3.sort_values(by=['pub_date', '_id'])\n",
        "\n",
        "    d3['keyword'] = d3.keyword.apply(lambda x: extr_keywords_step1(x))\n",
        "\n",
        "    keyword_table = d3[['keyword', 'section', '_id']]\n",
        "    table = keyword_table.groupby(by=['keyword', 'section']).count()\n",
        "    table = table.reset_index()\n",
        "    table.columns = ['keyword', 'section', 'counts']\n",
        "    return table\n",
        "\n",
        "\n",
        "def create_keyword_table(table, threshold, article_amount):\n",
        "    \"\"\"\n",
        "    table: table of keywords where one keyword can have multiply rows, if it appeared in different sections\n",
        "    function reduces this table to keyword_table, where each keyword only appears once and section is the most likely\n",
        "    section (if section is more frequent than threshold value), if no section stands out, tag as '*UNSPECIFIC*'\n",
        "    :param table: table of keywords\n",
        "    :param threshold: to what percentage keyword needs to appear in one section, that this section overweights\n",
        "    the others\n",
        "    :param article_amount: amount of articles of full data set, used to calculate frequency of keywords\n",
        "    :return: table of keywords where every keyword only appears once\n",
        "    \"\"\"\n",
        "    keyword_table = pd.DataFrame([['keyword', 'name', 'value', 0, 'section']],\n",
        "                                 columns=['keyword', 'name', 'value', 'counts', 'section'])\n",
        "    for i, kw in enumerate(table.keyword.unique()):\n",
        "        if i%100 == 0: print(str(i) + ' / 64537')\n",
        "\n",
        "        entries = table[table.keyword == kw]\n",
        "        entries_comb = entries.groupby(by=['keyword', 'section']).sum()\n",
        "        max_count = entries_comb.max()[0]\n",
        "        total_counts = entries_comb.sum()[0]\n",
        "        if max_count >= threshold*total_counts:\n",
        "            section = entries_comb.idxmax()[0][1]\n",
        "            # idx = entries['counts'].idxmax()\n",
        "            # section = table.loc[idx, 'section']\n",
        "        else:\n",
        "            section = '*UNSPECIFIC*'\n",
        "        new_row = pd.DataFrame(data=  [[ kw,        kw[0],  kw[1],   total_counts, section]],\n",
        "                               columns=['keyword', 'name', 'value', 'counts',     'section'])\n",
        "        keyword_table = keyword_table.append(new_row)\n",
        "        keyword_table['id'] = range(0, keyword_table.shape[0])\n",
        "        keyword_table['prob'] = np.log(keyword_table.counts / article_amount)\n",
        "    keyword_table = keyword_table[1:]\n",
        "\n",
        "    # weight for how many edges we reduce later\n",
        "    idf = np.log(article_amount / keyword_table.counts)\n",
        "    keyword_table['idf'] = idf / max(idf)\n",
        "\n",
        "    return keyword_table\n",
        "\n",
        "\n",
        "def extr_keywords(field, table_keywords):\n",
        "    \"\"\"\n",
        "    translate keywords structure as it comes from api to list of keywords ids (ids from table_keywords)\n",
        "    :param field: article keywords as it comes from api\n",
        "    :param table_keywords: table of keywords (created by create_keyword_table)\n",
        "    :return: list of keyword ids\n",
        "    \"\"\"\n",
        "    keyword_list = list()\n",
        "    for keyword in field:\n",
        "        try:\n",
        "            id = table_keywords.id[\n",
        "                (table_keywords.name == keyword['name']) &\n",
        "                (table_keywords.value == keyword['value'])]._get_values(0)\n",
        "            keyword_list.append(id)\n",
        "        except IndexError:\n",
        "            pass\n",
        "    return keyword_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS1W30uYGXOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def main_articles_keywords():\n",
        "    for year in ['2016', '2017', '2018']:\n",
        "        # get and save articles\n",
        "        for m in range(1,13):\n",
        "            month = str(m)\n",
        "            get_data(year, month)\n",
        "\n",
        "        # load articles, clean them\n",
        "        # concat dfs to df_year and then clean and translate keywords\n",
        "        for m in range(1, 13):\n",
        "            month = str(m)\n",
        "            if len(month) == 1:\n",
        "                month = '0' + month\n",
        "            suffix = year + \"_\" + month\n",
        "            print(suffix)\n",
        "\n",
        "            with open(cwd + \"/data/archive/articles_\" + suffix + \".pickle\", \"rb\") as f:\n",
        "                df_new = pickle.load(f)\n",
        "\n",
        "            if m == 1:\n",
        "                df_year = df_new\n",
        "            else:\n",
        "                df_year = pd.concat([df_year, df_new], ignore_index=True)\n",
        "\n",
        "        with open(cwd + \"/data/archive/articles_\" + year + \".pickle\", \"rb\") as f:\n",
        "            df_year = pickle.load(f)\n",
        "\n",
        "        print(df_year.shape)\n",
        "        df_year = clean_articles(df=df_year, word_count=20)\n",
        "        df_year = clean_sections(df_year)\n",
        "        # drop sections that are not interesting for keyword-analysis\n",
        "        df_year = df_year[~(df_year['section'] == '*DELETE*')]\n",
        "        print(df_year.shape)\n",
        "\n",
        "        with open(cwd + \"/data/archive/articles_\" + year + \"_clean.pickle\", \"wb\") as f:\n",
        "            pickle.dump(df_year, f)\n",
        "\n",
        "        # create keyword table for one year\n",
        "        table_year = create_keyword_table_partial(df_year)\n",
        "        with open(cwd + \"/data/table_keywords_partial_\" + year + \".pickle\", \"wb\") as f:\n",
        "            pickle.dump(table_year, f)\n",
        "\n",
        "    # combine keyword tables of singel years to full keyword table\n",
        "    for i, year in enumerate(['2016', '2017', '2018']):\n",
        "\n",
        "        with open(cwd + \"/data/archive/articles_\" + year + \"_clean.pickle\", \"rb\") as f:\n",
        "            df_year = pickle.load(f)\n",
        "        with open(cwd + \"/data/table_keywords_partial_\" + year + \".pickle\", \"rb\") as f:\n",
        "            table_year = pickle.load(f)\n",
        "\n",
        "        if i == 0:\n",
        "            table = table_year\n",
        "            df = df_year\n",
        "        else:\n",
        "            table = pd.concat([table, table_year], ignore_index=True)\n",
        "            df = pd.concat([df, df_year], ignore_index=True)\n",
        "        print(df.shape, table.shape)\n",
        "\n",
        "    with open(cwd + \"/data/archive/df_16-18.pickle\", \"wb\") as f:\n",
        "        pickle.dump(df, f)\n",
        "    with open(cwd + \"/data/archive/df_16-18.pickle\", \"rb\") as f:\n",
        "        df = pickle.load(f)\n",
        "\n",
        "    article_amount = df.shape[0]\n",
        "\n",
        "    # combine keyword_tables from different years (counts, idf, major section)\n",
        "    table_keywords = create_keyword_table(table, 0.35, article_amount)\n",
        "\n",
        "    with open(cwd + \"/data/table_keywords_16-18.pickle\", \"wb\") as f:\n",
        "        pickle.dump(table_keywords, f)\n",
        "\n",
        "    # use this table to translate keyword to ids\n",
        "    df['keywords'] = df.keywords.apply(lambda x: extr_keywords(x, table_keywords))\n",
        "    with open(cwd + \"/data/df_16-18.pickle\", \"wb\") as f:\n",
        "        pickle.dump(df, f)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk0Ipa8CDPnt",
        "colab_type": "text"
      },
      "source": [
        "## The Graph\n",
        "\n",
        "(Download folder [here](https://drive.google.com/open?id=1OIffBrPUZ9WZZCbGsj8HEwXQU-ANojZv) and open index.html in browser, e.g. firefox, navigate to file via file:///home/.../keyword-graph/index.html)\n",
        "\n",
        "### Nodes\n",
        "Each keyword is represented as a node and the color indicates which section the keyword appeared in the most, but only if it appears there in more than 35% of the occurrences. Otherwise we gave the tag 'Unspecific' (color: Lavender). 'Unknown' (color: Beige) is the tag for keywords that did not appear in articles that were assigned to a section (see paragraph about Section).\n",
        "\n",
        "### Edges\n",
        "Edges represent the connection between keywords that appeared together. Weight of the edges is the sum of the conditional probabilities. The probability is defined as \"fraction of articles that have this keyword\". \n",
        "$$\n",
        "W(A, B) = P (A|B) + P (B|A) = \\frac{P(A \\cap B)}{P(B)} + \\frac{P(B \\cap A)}{P(A)}\n",
        "$$\n",
        "In words: The conditional probability that keyword A appears in an article that contains keyword B is equal to the probability that keywords A and B appear in the same article divided by the probability with which keyword B appears. \n",
        "\n",
        "For example:\n",
        "$$\n",
        "W(\\text{'Musk, Elon'}, \\text{'Boring Company'}) \\\\\n",
        "= P(\\text{'Musk, Elon'} | \\text{'Boring Company'}) +  P(\\text{'Boring Company'} | \\text{'Musk, Elon'}) \\\\\n",
        "= 85.71 \\% + 3.35 \\% = 89,06 \\%\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pocwnwOgG_R5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def keyword_edges(field):\n",
        "    \"\"\"\n",
        "    creates list of edges between keywords\n",
        "    :param field: list of keyword ids\n",
        "    :return: list of edges\n",
        "    \"\"\"\n",
        "    field.sort()\n",
        "    edges = []\n",
        "    for subset in combinations(field, 2):\n",
        "        edge = str(subset[0]) + ',' + str(subset[1])\n",
        "        edges.append(edge)\n",
        "    return edges\n",
        "\n",
        "\n",
        "def edge_weight(edges_row, table_keywords):\n",
        "    \"\"\"\n",
        "    calculates weight of edge based on conditional probability.\n",
        "    In the beginning pro(edges, table_keywords) babilities in log-scale, for weight translated to normal scale\n",
        "    :param edges_row: one row from edges table (one edge with information)\n",
        "    :param table_keywords: keywords DataFrame\n",
        "    :return: weight of edge\n",
        "    \"\"\"\n",
        "    p1 = (edges_row.prob - table_keywords[table_keywords.id == edges_row.Target].prob).get_values()[0]\n",
        "    p2 = (edges_row.prob - table_keywords[table_keywords.id == edges_row.Source].prob).get_values()[0]\n",
        "    p1, p2 = np.exp(p1), np.exp(p2)\n",
        "    p = (p1 + p2)*100\n",
        "    return p\n",
        "\n",
        "\n",
        "def edges_nodes(article_keywords, table_keywords, article_amount):\n",
        "    \"\"\"\n",
        "    creates edges and nodes of the article keyword\n",
        "    :param article_keywords: keywords of articles, every article has a list of keywords\n",
        "    :param table_keywords: keywords DataFrame\n",
        "    :param article_amount: amount of articles (same as rows in article_keywords)\n",
        "    :return: edges DataFrame, nodes DataFrame\n",
        "    \"\"\"\n",
        "    edges_list = article_keywords.apply(lambda x: keyword_edges(x)).tolist()  # each article has a list of keywords\n",
        "    edges_df = pd.Series(list(chain.from_iterable(edges_list)))  # write everything in one list\n",
        "    edges_counts = edges_df.value_counts()\n",
        "\n",
        "    edges = pd.DataFrame([x.split(',') for x in edges_counts.index], columns=['keyword_1', 'keyword_2'])\n",
        "    edges['Source'] = edges.keyword_1.apply(lambda x: int(x))\n",
        "    edges['Target'] = edges.keyword_2.apply(lambda x: int(x))\n",
        "    edges['Counts'] = edges_counts.reset_index()[0]\n",
        "\n",
        "    e = edges[['Source', 'Target', 'Counts']]\n",
        "\n",
        "    # only keep edges where both Source and Target are in table_keywords\n",
        "    e_red = e[e.Source.isin(table_keywords.id) & e.Target.isin(table_keywords.id)]\n",
        "\n",
        "    e_red['prob'] = np.log(e_red.Counts/article_amount)\n",
        "    e_red['Weight'] = e_red.apply(lambda x: edge_weight(x, table_keywords), axis=1)\n",
        "\n",
        "    t = table_keywords[['id', 'section', 'value']]\n",
        "    ids_1 = e_red.Source.value_counts().index.get_values().tolist()  # unique ids in Source\n",
        "    ids_2 = e_red.Target.value_counts().index.get_values().tolist()  # unique ids in Target\n",
        "    mask = [any(y) for y in zip(t.id.isin(ids_1), t.id.isin(ids_2))]  # if id was either in Source or in Target or both\n",
        "    n = t[mask]\n",
        "    n.columns = ['id', 'Section', 'Label']\n",
        "\n",
        "    return e_red, n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wbn5c_GHJpe",
        "colab_type": "text"
      },
      "source": [
        "### Reducing The Number Of Edges And Nodes\n",
        "\n",
        "If we consider the data of all 3 years, we get around 65.000 nodes and 800.000 edges, which would make our graph very difficult to grasp. Therefore we tried to reduce the size of the graph, while still keeping the all of the interesting nodes and edges.\n",
        "\n",
        "First, for each node we only took the most frequent 35% of its' edges to other nodes, but only, if the edge is mutually in the top 35% of both nodes (sort of like mutual K-nearest neighbours). With the 35%-rule, Trump can keep 849 nodes, but he is also included in the top 35% of 1951 other nodes. If we now only consider the mutual top 35%, Trump is left with 847 edges which means that two of his edges did not appear in the top-35% of the other edges. \n",
        "\n",
        "Compared to a clean cut of the 35% most frequent edges, this node-wise-35% assures that we also keep the nodes and edges in less common topics that would otherwise disappear from the graph. \n",
        "\n",
        "So far we made sure that the less common topics also stay in the graph. Another problem that leads to a messy graph is, that some keywords appear in disproportionately many articles compared to the others. This mainly touches hyper-keywords like \"Politics and Government\", \"United States\" and \"Trump, Donald J\" (of course). Therefore the Inverse Document Frequency value (idf-value) is used to reduce heaviness of the nodes. Instead of keeping the top 35% of the edges as proposed above, we only keep idf-value*35% (Trump can then only keep 7% of his edges) the idf-value is caluclated as below and then normalized to the maximum value so that we get values between 1 and 0, where 1: keyword appears the least often, 0: keyword appears in every article.\n",
        "$$\n",
        "\\text{idf}(k) = log \\left(\\frac{\\# articles}{\\# \\text{articles with keyword } k}\\right)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaXDuDHWHDyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_edges(edges, nodes, percentage, table_keywords, min_edges):\n",
        "    \"\"\"\n",
        "    reduces the edges according to following:\n",
        "    - only keep edges that are in top mutual 'percentage'% edges of their nodes\n",
        "    - only keep nodes that have at least min_edges edges\n",
        "    :param edges: edges DataFrame\n",
        "    :param nodes: nodes DataFrame\n",
        "    :param percentage: cutoff precentage\n",
        "    :param table_keywords: keywords DataFrame\n",
        "    :param min_edges: minimum amount of edges per node, included\n",
        "    :return: lower dimensional edges DataFrame, lower dimensional nodes DataFrame\n",
        "    \"\"\"\n",
        "    # find top x% of edges to each node\n",
        "    # matrix of edges, nodes*nodes\n",
        "    mat = np.zeros([nodes.id.max()+1, nodes.id.max()+1])\n",
        "    for keyword_id in nodes.id:\n",
        "        # the other keywords that keyword_id is connected to\n",
        "        connected_t = edges[edges.Source == keyword_id][['Target', 'Counts']]\n",
        "        connected_t.columns = ['Node', 'Counts']\n",
        "        connected_s = edges[edges.Target == keyword_id] [['Source', 'Counts']]\n",
        "        connected_s.columns = ['Node', 'Counts']\n",
        "\n",
        "        total_connections = (connected_s).append(connected_t)\n",
        "        idf = table_keywords.idf[table_keywords.id == keyword_id]\n",
        "        max_edges = math.ceil(total_connections.shape[0]*percentage*idf)\n",
        "        tc = total_connections.sort_values(by='Counts', ascending=False)\n",
        "        tc = tc[:max_edges]\n",
        "\n",
        "        # entry = 1 if edge is in top x% of row-node\n",
        "        mat[keyword_id, tc.Node.tolist()] = 1\n",
        "\n",
        "    # only keep the edges that are in top x% of row-node AND column-node\n",
        "    keep_edges = dict()\n",
        "    for keyword_id in nodes.id:\n",
        "        keyword_has = mat[keyword_id, :]\n",
        "        keyword_appears_in = mat[:, keyword_id]\n",
        "\n",
        "        l1 = pd.Series(keyword_has).nonzero()[0].tolist()\n",
        "        l2 = pd.Series(keyword_appears_in).nonzero()[0].tolist()\n",
        "        intersection = set(l1) - (set(l1) - set(l2))\n",
        "        dict_update = {keyword_id: intersection}\n",
        "        keep_edges.update(dict_update)\n",
        "\n",
        "    mask = []\n",
        "    for idx in range(0, edges.shape[0]):\n",
        "        mask.append(edges.Target[idx] in keep_edges[edges.loc[idx].Source])\n",
        "\n",
        "    edges_reduced = edges[mask]\n",
        "\n",
        "    # remove the nodes that are not left after the x% filtering\n",
        "    s = edges_reduced.Source.value_counts()\n",
        "    t = edges_reduced.Target.value_counts()\n",
        "    st = pd.merge(pd.DataFrame(s), pd.DataFrame(t), left_index=True, right_index=True, how='outer').fillna(0)\n",
        "\n",
        "    mask = st.index.tolist()\n",
        "\n",
        "    nodes.index = nodes.id.tolist()\n",
        "    nodes_reduced = nodes.loc[mask]\n",
        "\n",
        "    # delete nodes that only have one edge\n",
        "    s = edges_reduced.Source.value_counts()\n",
        "    t = edges_reduced.Target.value_counts()\n",
        "\n",
        "    st = pd.merge(pd.DataFrame(s), pd.DataFrame(t), left_index=True, right_index=True, how='outer').fillna(0)\n",
        "    st['counts'] = st.Source + st.Target\n",
        "\n",
        "    # drop nodes that don't have enough edges\n",
        "    mask = (st.counts > min_edges)\n",
        "    idx = st[mask].index.get_values().tolist()\n",
        "    nodes_reduced = nodes_reduced[nodes_reduced.id.isin(idx)]\n",
        "\n",
        "    # drop edges where we had one of those nodes\n",
        "    mask = [all(tup) for tup in zip(edges_reduced.Source.isin(idx), edges_reduced.Target.isin(idx))]\n",
        "\n",
        "    edges_reduced = edges_reduced[mask]\n",
        "\n",
        "    return edges_reduced, nodes_reduced\n",
        "\n",
        "\n",
        "  \n",
        "def translate_id(table_keywords, edges, nodes):\n",
        "    \"\"\"\n",
        "    resets the keyword id in table_keywords to index of this table, in case some of the rows were deleted (ids would be missing)\n",
        "    renames ids in edges and nodes accordingly\n",
        "    :param table_keywords: keywords DataFrame\n",
        "    :param edges: edges DataFrame\n",
        "    :param nodes: nodes DataFrame\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    tr = pd.DataFrame(table_keywords.id)\n",
        "    tr['id_new'] = tr.index\n",
        "\n",
        "    edges_Source = pd.merge(edges, tr, left_on='Source', right_on='id')\n",
        "    edges_Source.columns = ['Source_old', 'Target', 'Counts', 'prob', 'Weight', 'id', 'Source']\n",
        "    edges_Target = pd.merge(edges_Source, tr, left_on='Target', right_on='id')\n",
        "    edges_Target.columns = ['Source_old', 'Target_old', 'Counts', 'prob', 'Weight', 'id_x', 'Source', 'id_y', 'Target']\n",
        "    edges = edges_Target[['Source', 'Target', 'Counts', 'prob', 'Weight']]\n",
        "\n",
        "    nodes_new = pd.merge(nodes, tr, left_on='id', right_on='id')\n",
        "    nodes_new.columns = ['id_old', 'Section', 'Label', 'id']\n",
        "    nodes = nodes_new[['id', 'Section', 'Label']]\n",
        "\n",
        "    table_keywords.id = tr.id_new\n",
        "    return table_keywords, edges, nodes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br4mYQm1HPx-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def main_keywordgraph():\n",
        "\n",
        "    with open(cwd + \"/data/table_keywords_16-18.pickle\", \"rb\") as f:\n",
        "        table_keywords = pickle.load(f)\n",
        "\n",
        "    with open(cwd + \"/data/df_16-18.pickle\", \"rb\") as f:\n",
        "        df = pickle.load(f)\n",
        "\n",
        "    article_amount = df.shape[0]\n",
        "    keywords = df.keywords\n",
        "\n",
        "    # only use keywords that will be relevant for us later,\n",
        "    # because will sort out less frequent ones in reduce_edges anyways\n",
        "    min_edges = 2\n",
        "    percentage = 0.35\n",
        "    table_keywords = table_keywords[table_keywords.counts >= min_edges / percentage]\n",
        "    table_keywords.index = range(0, table_keywords.shape[0])\n",
        "\n",
        "\n",
        "    edges, nodes = edges_nodes(keywords, table_keywords, article_amount)\n",
        "    print(edges.shape, nodes.shape)\n",
        "\n",
        "    with open(cwd + \"/data/edges_16-18.pickle\", \"wb\") as f:\n",
        "        pickle.dump(edges, f)\n",
        "    with open(cwd + \"/data/nodes_16-18.pickle\", \"wb\") as f:\n",
        "        pickle.dump(nodes, f)\n",
        "\n",
        "\n",
        "    # reduce_edges\n",
        "    table_keywords, edges, nodes = translate_id(table_keywords, edges, nodes)\n",
        "\n",
        "    edges_reduced, nodes_reduced = reduce_edges(edges, nodes, percentage, table_keywords, min_edges)\n",
        "    print(edges_reduced.shape, nodes_reduced.shape)\n",
        "\n",
        "    series = '02'\n",
        "    name = 'idf-mutual_16-18_3'\n",
        "    nodes_reduced.to_csv(cwd + '/data/gephi/' + series + 'nodes_' + name + '.csv', sep=';', index=False)\n",
        "    edges_reduced.to_csv(cwd + '/data/gephi/' + series + 'edges_' + name + '.csv', sep=';', index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM00kDmgCjYf",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "The full graph can be explored here: (Download folder [here](https://drive.google.com/open?id=1OIffBrPUZ9WZZCbGsj8HEwXQU-ANojZv) and open index.html in browser, e.g. firefox, navigate to file via file:///home/.../keyword-graph/index.html)\n",
        "\n",
        "\n",
        "It was created with Gephi (with layout algorithm: Force Atlas 2) and exported with the sigma exporter extension.\n",
        "\n",
        "The obvious problem with visualizing network graphs is that the graph itself is multidimensional, whereas the visualization can only capture two dimensions. So the visualization inevitably swollows some of the relationships in the network. \n",
        "\n",
        "Inour case the network is nicely structured in sections where the topics do not overlap much. \n",
        "For example the keywords of the Sports section mainly refer to the Name of the players, their team and the league, which are all unique for each discipline in Sports. This results in clusters that represent the different disciplines (Baseball, Basketball, Soccer, Football, Golf, Tennis)\n",
        "\n",
        "https://drive.google.com/open?id=1yXb25nJC8Ry4Bxgl-WZ2N-vts9lNqdZZ\n",
        "\n",
        "Also the Culture-section is clean enough to form sub clusters according to the areas Music, Theater, Movies, Books. \n",
        "The advantage of those two sections (Sports and Culture) is, that they have clear sub-sections within and also do not overlap too much with the other sections. Of course Sports has edges to World, that's why it is located next to it, but the edges are not as strong as the edges within Sports itself. \n",
        "\n",
        "https://drive.google.com/open?id=1tzXKhmIMfg9AT16vldnNaGpEI5p54iiu\n",
        "\n",
        "\n",
        "The problem of dimensionality reduction becomes obvious in sections where:\n",
        "- Topics overlap much inside one Section (for example the node 'elections' is placed somewhere in the middle of the World section, because it is connected to many countries.)\n",
        "- Sections that do not have clear borders to other sections. Many of the topics that are covered in the New York section, could also be part of U.S. or Business&Technology. \n",
        "\n",
        "So we find the sections: World, U.S., New York, Business&Technology very close together in the middle of the graph. The nodes are still pushed towards one corner or the other acording to their section, and there is still some structure inside the sections (e.g. Israel-Palestinians-Jerusalem-Netanyahu-... all next to each other). But the sections overlap extremely and we do not see nice sub-clusters like in the sections Sports and Culture. \n",
        "\n",
        "https://drive.google.com/open?id=1-fkdz7I2JpFXKMlh5R9kvc552Ni045TN\n",
        "\n",
        "Keywords with tag 'Unspecific': \n",
        "When we assigned the keywords to the section that they appeared in the most, we gave the tag 'Unspecific' to those keywords that appear almost equally in at least two sections and therefore cannot be assigned to one section specificially. \n",
        "The behavior of those keywords also shows the problem of the section-overlap in certain areas. The 'Unspecific' keywords mainly appear in the middle, of the graph, the blurry part (World, U.S. Business&Technology, ...) but almost do not appear in the well clustered part of the graph (Sports, Culture). "
      ]
    }
  ]
}